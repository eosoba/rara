{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We’ll take another look at the boston real estate dataset. \n",
    "\n",
    "## Data Schema\n",
    "\n",
    "Lat: latitude of home\n",
    "\n",
    "Lon: longitude of home\n",
    "\n",
    "Crim: per capita crime rate by town \n",
    "\n",
    "Zn: proportion of residential land zoned for lots over 25,000 sq.ft. \n",
    "\n",
    "Indus : proportion of non-retail business acres per town\n",
    "\n",
    "Chas: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) \n",
    "\n",
    "Nox: nitrogen oxides concentration (parts per 10 million) \n",
    "\n",
    "Rm: average number of rooms per dwelling \n",
    "\n",
    "Age: proportion of owner-occupied units built prior to 1940 \n",
    "\n",
    "Dis: weighted mean of distances to five Boston employment centres \n",
    "\n",
    "Rad: index of accessibility to radial highways \n",
    "\n",
    "Tax: full-value property-tax rate per $10,000 \n",
    "\n",
    "Ptratio: pupil-teacher ratio by town \n",
    "\n",
    "Black: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town \n",
    "\n",
    "Lstat:  lower status of the population (percent) \n",
    "\n",
    "Medv: median value of owner-occupied homes in $1000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I (10pts)\n",
    "# Perform logistic regression on “LAT” and “LON” columns to predict if a home is in the top or bottom 50th percentile of home value. \n",
    "\n",
    "## Follow the example in the logistic regression demo to do this task.\n",
    "- remember to shift-enter to run cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data, name it boston\n",
    "'''\n",
    "load data here\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'median_medv' variable = the median of the medv column, analogous to the med_crime variable created in lecture\n",
    "'''\n",
    "create median_medv here\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create high_price column as follows. Rows that have values greater than the MEDV median will be 1 and other rows will be 0, like so:\n",
    "\n",
    "boston['high_price']=boston['MEDV'].map(lambda x: 1 if (x>median_medv)  else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an array of training columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cols = ['LON', 'LAT' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do a test/train data split. This time, we will do it differently from the lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,y_train,y_test =train_test_split(boston[train_cols],boston['high_price'],test_size=0.20, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the contents and the shape of X_train, X_test, y_train, y_test with the head and shape method, for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape\n",
    "# y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a logistic regression model:\n",
    "logit = sm.Logit(y_train, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What’s the accuracy, precision, and recall of your model on the training data and the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "work answer here\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II (10pts)\n",
    "\n",
    "# It’s always a good idea to scale your non-categorical columns, so that they all lie roughly on the same scale. \n",
    "- By scaling I mean, making sure all the columns have zero-mean and unit variance.  \n",
    "- We could do this directly, e.g., train[‘LAT’] = (train[‘LAT’] - train[‘LAT’].mean())/train[‘LAT’].std().\n",
    "- But luckily, there are libraries that will do this for you. In order to do this, we need to import a new library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following code to scale columns\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "scaled_train = scaler.transform(X_train)\n",
    "scaled_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you tried, scaled_train.head(), you will see an error. \n",
    "- This is because the scaler converted the dataframe to a numpy array. \n",
    "- Numpy arrays are more low-level objects which will work fine for model training \n",
    "- but they are less easy to work with. We will however work with them as numpy arrays for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your model again, using the new scaled columns:\n",
    "new_logit = sm.Logit(y_train, scaled_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What’s the new accuracy, precision, and recall of your model using the scaled columns on your training set and test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "work answer here\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more columns to the training columns.\n",
    "training_cols = ['LON', 'LAT' , 'INDUS', 'CRIM']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the data with the new set of columns. \n",
    "- It’s even more important to scale here \n",
    "- because INDUS column and the CRIM columns lie on different scales from the LAT, LON columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "scale data here\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model again with the new set of training columns.\n",
    "\n",
    "'''\n",
    "retrain model here\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What’s the new accuracy, precision, and recall of your model on the training data and the test data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "work data here\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
