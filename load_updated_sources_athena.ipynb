{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import warnings \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as f\n",
    "import subprocess\n",
    "import datetime\n",
    "from pyspark.sql.window import Window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import HiveContext\n",
    "hive = HiveContext(sc) \n",
    "hive.setConf(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-23\n"
     ]
    }
   ],
   "source": [
    "RUN_DATE = datetime.date.today()\n",
    "print(RUN_DATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load updated transaction data directly from source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_trx = \"s3://formation-samba-us-west-2-seabright-samba-sources/transaction/historical/load_dt=2020-09-15\"\n",
    "hist_trx = spark.read.csv(hist_trx,header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_trx =hist_trx.withColumn('load_date',f.lit('2020-09-15').cast('date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id_customer='000159ba66919ddf7c703d1b0ff79fb2', clube=None, clube_plano=None, tier='Ouro', alias_name='G3', data_processamento='2017-09-19T00:00', data_transacao='2017-09-01T00:00', safra='201709', id_transacao='1-109DS6BM', nome_parceiro='GOL LINHAS AEREAS', tipo_parceiro='GOL', tipo_produto='CREDITO RETROATIVO', tipo_milhas='Milhas', data_voo='2017-09-01T00:00', qtd_bilhetes=None, nr_voo='1603', pnr='AMHBRK', od='BPSROS', aeroporto_origem='BPS', continente_origem=None, aeroporto_destino='ROS', continente_destino=None, tipo_viagem=None, teto_diamante=None, viaje_facil=None, voucher='0', assento='0', bagagem='0', classe=None, tp_tarifa=None, parte_milhas=None, parte_money=None, valor_money=None, volume_milhas='2209', canal_operacao='DEMAIS CANAIS', tp_transacao='ACCRUAL', load_date=datetime.date(2020, 9, 15))]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_trx.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|       yoo|\n",
      "+----------+\n",
      "|2020-09-07|\n",
      "|2020-09-06|\n",
      "|2020-09-05|\n",
      "|2020-09-04|\n",
      "|2020-09-03|\n",
      "|2020-09-02|\n",
      "|2020-09-01|\n",
      "|2020-08-31|\n",
      "|2020-08-30|\n",
      "|2020-08-29|\n",
      "|2020-08-28|\n",
      "|2020-08-27|\n",
      "|2020-08-26|\n",
      "|2020-08-25|\n",
      "|2020-08-24|\n",
      "|2020-08-23|\n",
      "|2020-08-22|\n",
      "|2020-08-21|\n",
      "|2020-08-20|\n",
      "|2020-08-19|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hist_trx.select(f.to_timestamp('data_transacao').cast('date').alias('yoo')).distinct().sort(f.col('yoo').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_trx = \"s3://formation-samba-us-west-2-seabright-samba-sources/transaction/v1/*/*.csv\"\n",
    "daily_trx = spark.read.csv(daily_trx,header=True,sep=\",\")\n",
    "daily_trx = daily_trx.withColumn('filename',f.input_file_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_trx = daily_trx.withColumn('data_processamento',f.to_timestamp('data_processamento').cast('date'))\\\n",
    "            .withColumn('data_transacao',f.to_timestamp('data_transacao').cast('date'))\\\n",
    "            .withColumn('data_voo',f.to_timestamp('data_voo'))\\\n",
    "            .withColumn('qtd_bilhetes',f.coalesce(f.col('qtd_bilhetes').cast('int'),f.lit(0)))\\\n",
    "            .withColumn('teto_diamante',f.coalesce(f.col('teto_diamante').cast('int'),f.lit(0)))\\\n",
    "            .withColumn('viaje_facil',f.coalesce(f.col('viaje_facil').cast('int'),f.lit(0)))\\\n",
    "            .withColumn('voucher',f.coalesce(f.col('voucher').cast('int'),f.lit(0)))\\\n",
    "            .withColumn('assento',f.coalesce(f.col('assento').cast('int'),f.lit(0)))\\\n",
    "            .withColumn('bagagem',f.coalesce(f.col('bagagem').cast('int'),f.lit(0)))\\\n",
    "            .withColumn('parte_milhas',f.coalesce(f.col('parte_milhas').cast('long'),f.lit(0)))\\\n",
    "            .withColumn('parte_money',f.coalesce(f.col('parte_money').cast('long'),f.lit(0)))\\\n",
    "            .withColumn('valor_money',f.coalesce(f.col('valor_money').cast('long'),f.lit(0)))\\\n",
    "            .withColumn('volume_milhas',f.coalesce(f.col('volume_milhas').cast('long'),f.lit(0)))\\\n",
    "            .withColumn('tp_transacao',f.when(f.col('tp_transacao')=='REDEPMTION AIRLINE','REDEMPTION AIRLINE')\n",
    "                                    .otherwise(f.col('tp_transacao')))\\\n",
    "            .withColumn('load_date',f.split(f.input_file_name(),\"/\").getItem(5).cast('date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_trx = hist_trx\\\n",
    "            .withColumn('data_processamento',f.to_timestamp('data_processamento').cast('date'))\\\n",
    "            .withColumn('data_transacao',f.to_timestamp('data_transacao').cast('date'))\\\n",
    "            .withColumn('data_voo',f.to_timestamp('data_voo'))\\\n",
    "            .withColumn('qtd_bilhetes',f.coalesce(f.col('qtd_bilhetes').cast('int'),f.lit(0)))\\\n",
    "            .withColumn('teto_diamante',f.coalesce(f.col('teto_diamante').cast('int'),f.lit(0)))\\\n",
    "            .withColumn('viaje_facil',f.coalesce(f.col('viaje_facil').cast('int'),f.lit(0)))\\\n",
    "            .withColumn('voucher',f.coalesce(f.col('voucher').cast('int'),f.lit(0)))\\\n",
    "            .withColumn('assento',f.coalesce(f.col('assento').cast('int'),f.lit(0)))\\\n",
    "            .withColumn('bagagem',f.coalesce(f.col('bagagem').cast('int'),f.lit(0)))\\\n",
    "            .withColumn('parte_milhas',f.coalesce(f.col('parte_milhas').cast('long'),f.lit(0)))\\\n",
    "            .withColumn('parte_money',f.coalesce(f.col('parte_money').cast('long'),f.lit(0)))\\\n",
    "            .withColumn('valor_money',f.coalesce(f.col('valor_money').cast('long'),f.lit(0)))\\\n",
    "            .withColumn('volume_milhas',f.coalesce(f.col('volume_milhas').cast('long'),f.lit(0)))\\\n",
    "            .withColumn('tp_transacao',f.when(f.col('tp_transacao')=='REDEPMTION AIRLINE','REDEMPTION AIRLINE')\n",
    "                                    .otherwise(f.col('tp_transacao')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_trx.select('tp_transacao').distinct().show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(data_transacao=datetime.date(2020, 9, 7), count=21285),\n",
       " Row(data_transacao=datetime.date(2020, 9, 6), count=19808),\n",
       " Row(data_transacao=datetime.date(2020, 9, 5), count=220326),\n",
       " Row(data_transacao=datetime.date(2020, 9, 4), count=47821),\n",
       " Row(data_transacao=datetime.date(2020, 9, 3), count=41586),\n",
       " Row(data_transacao=datetime.date(2020, 9, 2), count=54635),\n",
       " Row(data_transacao=datetime.date(2020, 9, 1), count=37564),\n",
       " Row(data_transacao=datetime.date(2020, 8, 31), count=39645),\n",
       " Row(data_transacao=datetime.date(2020, 8, 30), count=22369),\n",
       " Row(data_transacao=datetime.date(2020, 8, 29), count=17593)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_trx.select('data_transacao').groupby('data_transacao').count().sort(f.col('data_transacao').desc()).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "cus_trans = daily_trx.filter(f.col('data_processamento') > f.lit('2020-09-08'))\\\n",
    ".union(hist_trx.filter(f.col('data_processamento') != f.lit('2029-07-10')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''drop table if exists clean.transactions''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " query='''  CREATE TABLE IF NOT EXISTS clean.transactions (\n",
    "   id_customer string ,\n",
    "    clube string ,\n",
    "    clube_plano string ,\n",
    "    tier string ,\n",
    "    alias_name string ,\n",
    "    data_processamento date ,\n",
    "    data_transacao date ,\n",
    "    safra string ,\n",
    "    id_transacao string ,\n",
    "    nome_parceiro string ,\n",
    "    tipo_parceiro string ,\n",
    "    tipo_produto string ,\n",
    "    tipo_milhas string ,\n",
    "    data_voo timestamp ,\n",
    "    qtd_bilhetes integer ,\n",
    "    nr_voo string ,\n",
    "    pnr string ,\n",
    "    od string ,\n",
    "    aeroporto_origem string ,\n",
    "    continente_origem string ,\n",
    "    aeroporto_destino string ,\n",
    "    continente_destino string ,\n",
    "    tipo_viagem string ,\n",
    "    teto_diamante integer ,\n",
    "    viaje_facil integer ,\n",
    "    voucher integer ,\n",
    "    assento integer ,\n",
    "    bagagem integer ,\n",
    "    classe string ,\n",
    "    tp_tarifa string ,\n",
    "    parte_milhas long ,\n",
    "    parte_money long ,\n",
    "    valor_money long ,\n",
    "    volume_milhas long ,\n",
    "    canal_operacao string ,\n",
    "    tp_transacao string )\n",
    "    PARTITIONED BY (load_date DATE)\n",
    "    STORED AS PARQUET\n",
    "    LOCATION 's3://formation-samba-us-west-2-seabright-samba-derived/clean/transactions'\n",
    "    '''\n",
    "spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_trx.filter(f.col('data_processamento') > f.lit('2020-09-08'))\\\n",
    ".drop('filename')\\\n",
    ".write.format(\"parquet\")\\\n",
    ".mode('append')\\\n",
    ".insertInto(\"clean.transactions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get customer transactions\n",
    "\n",
    "query = \"\"\" select max(load_date) from clean.transactions \"\"\"\n",
    "\n",
    "max_load_date = str(spark.sql(query).take(1)[0][0])\n",
    "print(max_load_date)\n",
    "\n",
    "\n",
    "cmd = \"aws s3 ls --recursive  s3://formation-samba-us-west-2-seabright-samba-sources/transaction/v1/\"\n",
    "wow = subprocess.check_output(cmd.split())\n",
    "bsn = 's3://formation-samba-us-west-2-seabright-samba-sources/'\n",
    "daily_trx = [bsn+a.decode('utf-8') for a in wow.split() if a.decode('utf-8').endswith('csv')]\n",
    "dates = [a.split('v1/')[1].split('/')[0] for a in daily_trx]\n",
    "\n",
    "\n",
    "ix = dates.index(max_load_date)\n",
    "trx_append = spark.read.csv(daily_trx[ix+1:],header=True)\n",
    "trx_append = trx_append.withColumn('filename',f.input_file_name())\\\n",
    "            .withColumn('data_processamento',f.to_timestamp('data_processamento').cast('date'))\\\n",
    "            .withColumn('data_transacao',f.to_timestamp('data_transacao').cast('date'))\\\n",
    "            .withColumn('data_voo',f.to_timestamp('data_voo'))\\\n",
    "            .withColumn('qtd_bilhetes',f.coalesce(f.col('qtd_bilhetes').cast('int'),f.lit(0)))\\\n",
    "            .withColumn('teto_diamante',f.coalesce(f.col('teto_diamante').cast('int'),f.lit(0)))\\\n",
    "            .withColumn('viaje_facil',f.coalesce(f.col('viaje_facil').cast('int'),f.lit(0)))\\\n",
    "            .withColumn('voucher',f.coalesce(f.col('voucher').cast('int'),f.lit(0)))\\\n",
    "            .withColumn('assento',f.coalesce(f.col('assento').cast('int'),f.lit(0)))\\\n",
    "            .withColumn('bagagem',f.coalesce(f.col('bagagem').cast('int'),f.lit(0)))\\\n",
    "            .withColumn('parte_milhas',f.coalesce(f.col('parte_milhas').cast('long'),f.lit(0)))\\\n",
    "            .withColumn('parte_money',f.coalesce(f.col('parte_money').cast('long'),f.lit(0)))\\\n",
    "            .withColumn('valor_money',f.coalesce(f.col('valor_money').cast('long'),f.lit(0)))\\\n",
    "            .withColumn('volume_milhas',f.coalesce(f.col('volume_milhas').cast('long'),f.lit(0)))\\\n",
    "            .withColumn('tp_transacao',f.when(f.col('tp_transacao')=='REDEPMTION AIRLINE','REDEMPTION AIRLINE')\n",
    "                                    .otherwise(f.col('tp_transacao')))\\\n",
    "            .withColumn('load_date',f.split(f.input_file_name(),\"/\").getItem(5).cast('date'))\n",
    "\n",
    "trx_append.groupby().agg(f.max('data_transacao'),f.min('data_transacao'),f.min('load_date'),f.max('load_date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trx_append.drop('filename')\\\n",
    ".write.format(\"parquet\")\\\n",
    ".mode('append')\\\n",
    ".insertInto(\"clean.transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|partition           |\n",
      "+--------------------+\n",
      "|load_date=2020-09-08|\n",
      "|load_date=2020-09-09|\n",
      "|load_date=2020-09-10|\n",
      "|load_date=2020-09-11|\n",
      "|load_date=2020-09-12|\n",
      "|load_date=2020-09-13|\n",
      "|load_date=2020-09-14|\n",
      "|load_date=2020-09-15|\n",
      "|load_date=2020-09-16|\n",
      "|load_date=2020-09-17|\n",
      "|load_date=2020-09-18|\n",
      "|load_date=2020-09-19|\n",
      "|load_date=2020-09-20|\n",
      "|load_date=2020-09-21|\n",
      "|load_date=2020-09-22|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''show partitions clean.transactions''').show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path =\"s3://formation-samba-us-west-2-seabright-samba-sources/demographic/v1/2020-08-28/Base_Demografica_Formation_20200828.csv\"\n",
    "demos_path = \"s3://formation-samba-us-west-2-seabright-samba-sources/demographic/v1/*/*.CSV\"\n",
    "cus_demos = spark.read.csv(demos_path,header=True,sep=\",\")\n",
    "cus_demos = cus_demos.filter(f.col('id_customer') != '411801aa193970bae7a6f641b6cc115b')\\\n",
    "                     .withColumn('load_date',f.split(f.input_file_name(),'/').getItem(5).cast('date'))\\\n",
    "                    .withColumn('filename',f.input_file_name())\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------+\n",
      "|filename                                                                                                                        |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------+\n",
      "|s3://formation-samba-us-west-2-seabright-samba-sources/demographic/v1/2020-04-01/Base_Demografica_Formation_20200402.CSV        |\n",
      "|s3://formation-samba-us-west-2-seabright-samba-sources/demographic/v1/2020-06-23/Base_Demografica_Formation_202000624.CSV       |\n",
      "|s3://formation-samba-us-west-2-seabright-samba-sources/demographic/v1/load_dt=2020-09-17/Base_Demografica_Formation_20200917.CSV|\n",
      "|s3://formation-samba-us-west-2-seabright-samba-sources/demographic/v1/2020-06-10/Base_Demografica_Formation_20200611.CSV        |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cus_demos.select('filename').distinct().show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =\"s3://formation-samba-us-west-2-seabright-samba-sources/demographic/v1/load_dt=2020-09-17/Base_Demografica_Formation_20200917.CSV\"\n",
    "cus_demos = spark.read.csv(path,header=True)\n",
    "hmm = cus_demos.withColumn('load_date',f.split(f.split(f.input_file_name(),'=').getItem(1),'/').getItem(0).cast('date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17729600"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " query='''  CREATE TABLE IF NOT EXISTS clean.customer_profile (\n",
    "       id_customer string ,\n",
    "     ind_ativo int ,\n",
    "     ind_cobranded int ,\n",
    "     clube_ativo string ,\n",
    "     plano string ,\n",
    "     tipo_plano string ,\n",
    "     data_adesao date ,\n",
    "     app_ativo int ,\n",
    "     dt_instalacao date ,\n",
    "     login_app string ,\n",
    "     dt_login_app string ,\n",
    "     login_site int ,\n",
    "     dt_login_site timestamp ,\n",
    "     preferencia int ,\n",
    "     dt_preenchimento date ,\n",
    "     cf_pai string ,\n",
    "     created_cf_pai string ,\n",
    "     cf_filho string ,\n",
    "     created_cf_filho string ,\n",
    "     estado_civil string ,\n",
    "     nacionalidade string ,\n",
    "     data_cadastro date ,\n",
    "     data_nascimento date ,\n",
    "     canal_cadastro string ,\n",
    "     cidade string ,\n",
    "     pais string ,\n",
    "     uf string ,\n",
    "     tier string ,\n",
    "     sexo string ,\n",
    "     saldo int ,\n",
    "     tempo_cadastro int ,\n",
    "     idade int ,\n",
    "     regiao string ,\n",
    "     grupo_engajamento string ,\n",
    "     renda_final double ,\n",
    "     rentabilidade double ,\n",
    "     grupo_rentabilidade string ,\n",
    "     open7_day string ,\n",
    "     open15_day string ,\n",
    "     open30_day string ,\n",
    "     flag_cell_phone int ,\n",
    "     flag_email integer ,\n",
    "     milheiros int ,\n",
    "     elegiveis int,\n",
    "     offer_credit_card string,\n",
    "     offer_radar_smiles string,\n",
    "     offer_buy_miles string )\n",
    "    PARTITIONED BY (load_date DATE)\n",
    "    STORED AS PARQUET\n",
    "    LOCATION 's3://formation-samba-us-west-2-seabright-samba-derived/clean/customer_profile'\n",
    "    '''\n",
    "spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = '''CREATE DATABASE clean\n",
    "location  's3://formation-samba-us-west-2-seabright-samba-derived/clean' \n",
    "'''\n",
    "spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm.write.format(\"parquet\")\\\n",
    ".mode('append')\\\n",
    ".insertInto(\"clean.customer_profile\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load updated digital actions directly from source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =\"s3://formation-samba-us-west-2-seabright-samba-sources/historical/Full_Table-2020-03-25/Base_Dig_Onboarding_Formation_FULL.csv\"\n",
    "hist_dig_trx = spark.read.csv(path,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s3://formation-samba-us-west-2-seabright-samba-sources/digital-action/v1/2020-09-14/Base_Dig_Onboarding_Formation.csv', 's3://formation-samba-us-west-2-seabright-samba-sources/digital-action/v1/2020-09-15/Base_Dig_Onboarding_Formation.csv', 's3://formation-samba-us-west-2-seabright-samba-sources/digital-action/v1/2020-09-16/Base_Dig_Onboarding_Formation.csv']\n"
     ]
    }
   ],
   "source": [
    "daily_dig_trx = \"s3://formation-samba-us-west-2-seabright-samba-sources/digital-action/v1/*/*.csv\"\n",
    "daily_dig_trx = spark.read.csv(daily_dig_trx,header=True,sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_dig_trx = daily_dig_trx.withColumn('update_date',f.to_timestamp('update_date').cast('date'))\n",
    "hist_dig_trx = hist_dig_trx.withColumn('update_date',f.to_timestamp('update_date').cast('date'))\n",
    "hist_dig_trx = hist_dig_trx.filter(f.col('update_date')<f.lit('2020-03-23'))\\\n",
    "              .withColumn('load_date',f.split(f.split(f.input_file_name(),\"/\").getItem(4),\"Full_Table-\").getItem(1).cast('date'))\n",
    "\n",
    "daily_dig_trx = daily_dig_trx.withColumn('load_date',f.split(f.input_file_name(),\"/\").getItem(5).cast('date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(update_date=datetime.date(2020, 3, 24)),\n",
       " Row(update_date=datetime.date(2020, 3, 23)),\n",
       " Row(update_date=datetime.date(2020, 3, 22)),\n",
       " Row(update_date=datetime.date(2020, 3, 21))]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_dig_trx.select('update_date').distinct().sort(f.col('update_date').desc()).take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(update_date=datetime.date(2020, 3, 24)),\n",
       " Row(update_date=datetime.date(2020, 3, 25)),\n",
       " Row(update_date=datetime.date(2020, 3, 26)),\n",
       " Row(update_date=datetime.date(2020, 3, 27))]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_dig_trx.select('update_date').distinct().sort(f.col('update_date')).take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "dig_trx = daily_dig_trx.union(hist_dig_trx).filter(f.col('id_customer')!='')\n",
    "dig_trx = dig_trx.withColumn('action_flag',f.regexp_replace(f.lower(f.col('action_flag')),\" \",\"_\"))\\\n",
    "                .withColumn('action_flag',f.when(f.col('action_flag') == 'filling_in_preferences',f.lit('fill_in_preferences'))\n",
    "                            .otherwise(f.col('action_flag')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|action_flag        |\n",
      "+-------------------+\n",
      "|login_app          |\n",
      "|update_profile     |\n",
      "|fill_in_preferences|\n",
      "|download_app       |\n",
      "|login_site         |\n",
      "|radar_smiles       |\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dig_trx.select('action_flag').distinct().show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-16\n"
     ]
    }
   ],
   "source": [
    "# get customer transactions\n",
    "\n",
    "query = \"\"\" select max(load_date) from clean.digital_actions \"\"\"\n",
    "\n",
    "max_load_date = str(spark.sql(query).take(1)[0][0])\n",
    "print(max_load_date)\n",
    "\n",
    "\n",
    "cmd = \"aws s3 ls --recursive  s3://formation-samba-us-west-2-seabright-samba-sources/digital-action/v1/\"\n",
    "wow = subprocess.check_output(cmd.split())\n",
    "bsn = 's3://formation-samba-us-west-2-seabright-samba-sources/'\n",
    "daily_trx = [bsn+a.decode('utf-8') for a in wow.split() if a.decode('utf-8').endswith('csv')]\n",
    "dates = [a.split('v1/')[1].split('/')[0] for a in daily_trx]\n",
    "\n",
    "\n",
    "ix = dates.index(max_load_date)\n",
    "trx_append = spark.read.csv(daily_trx[ix+1:],header=True)\n",
    "trx_append = trx_append\\\n",
    "                    .withColumn('update_date',f.to_timestamp('update_date').cast('date'))\\\n",
    "                    .withColumn('filename',f.input_file_name())\\\n",
    "                     .withColumn('action_flag',f.regexp_replace(f.lower(f.col('action_flag')),\" \",\"_\"))\\\n",
    "                    .withColumn('action_flag',f.when(f.col('action_flag') == 'filling_in_preferences',f.lit('fill_in_preferences')))\\\n",
    "                 .withColumn('load_date',f.split(f.input_file_name(),\"/\").getItem(5).cast('date'))        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+--------------+--------------+\n",
      "|max(update_date)|min(update_date)|min(load_date)|max(load_date)|\n",
      "+----------------+----------------+--------------+--------------+\n",
      "|      2020-09-23|      2020-09-16|    2020-09-17|    2020-09-22|\n",
      "+----------------+----------------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trx_append.groupby().agg(f.max('update_date'),f.min('update_date'),f.min('load_date'),f.max('load_date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.orderBy(\"update_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"Expression 'update_date#2190' not supported within a window function.;;\\nProject [id_customer#2184, update_date#2190, action_flag#2204, filename#2194, load_date#2209, lagy#2269]\\n+- Project [id_customer#2184, update_date#2190, action_flag#2204, filename#2194, load_date#2209, _w0#2270, lag(_w0#2270, 1, null) AS lagy#2269]\\n   +- Project [id_customer#2184, update_date#2190, action_flag#2204, filename#2194, load_date#2209, _w0#2270]\\n      +- Project [id_customer#2184, update_date#2190, action_flag#2204, filename#2194, load_date#2209, _w0#2270, _w0#2270]\\n         +- Window [update_date#2190 windowspecdefinition(update_date#2190 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS _w0#2270], [update_date#2190 ASC NULLS FIRST]\\n            +- Project [id_customer#2184, update_date#2190, action_flag#2204, filename#2194, load_date#2209]\\n               +- Project [id_customer#2184, update_date#2190, action_flag#2204, filename#2194, cast(split(input_file_name(), /)[5] as date) AS load_date#2209]\\n                  +- Project [id_customer#2184, update_date#2190, CASE WHEN (action_flag#2199 = filling_in_preferences) THEN fill_in_preferences END AS action_flag#2204, filename#2194]\\n                     +- Project [id_customer#2184, update_date#2190, regexp_replace(lower(action_flag#2186),  , _) AS action_flag#2199, filename#2194]\\n                        +- Project [id_customer#2184, update_date#2190, action_flag#2186, input_file_name() AS filename#2194]\\n                           +- Project [id_customer#2184, cast(to_timestamp('update_date, None) as date) AS update_date#2190, action_flag#2186]\\n                              +- Relation[id_customer#2184,update_date#2185,action_flag#2186] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o392.withColumn.\n: org.apache.spark.sql.AnalysisException: Expression 'update_date#2190' not supported within a window function.;;\nProject [id_customer#2184, update_date#2190, action_flag#2204, filename#2194, load_date#2209, lagy#2269]\n+- Project [id_customer#2184, update_date#2190, action_flag#2204, filename#2194, load_date#2209, _w0#2270, lag(_w0#2270, 1, null) AS lagy#2269]\n   +- Project [id_customer#2184, update_date#2190, action_flag#2204, filename#2194, load_date#2209, _w0#2270]\n      +- Project [id_customer#2184, update_date#2190, action_flag#2204, filename#2194, load_date#2209, _w0#2270, _w0#2270]\n         +- Window [update_date#2190 windowspecdefinition(update_date#2190 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS _w0#2270], [update_date#2190 ASC NULLS FIRST]\n            +- Project [id_customer#2184, update_date#2190, action_flag#2204, filename#2194, load_date#2209]\n               +- Project [id_customer#2184, update_date#2190, action_flag#2204, filename#2194, cast(split(input_file_name(), /)[5] as date) AS load_date#2209]\n                  +- Project [id_customer#2184, update_date#2190, CASE WHEN (action_flag#2199 = filling_in_preferences) THEN fill_in_preferences END AS action_flag#2204, filename#2194]\n                     +- Project [id_customer#2184, update_date#2190, regexp_replace(lower(action_flag#2186),  , _) AS action_flag#2199, filename#2194]\n                        +- Project [id_customer#2184, update_date#2190, action_flag#2186, input_file_name() AS filename#2194]\n                           +- Project [id_customer#2184, cast(to_timestamp('update_date, None) as date) AS update_date#2190, action_flag#2186]\n                              +- Relation[id_customer#2184,update_date#2185,action_flag#2186] csv\n\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:43)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:152)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\n\tat org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2258)\n\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2225)\n\tat sun.reflect.GeneratedMethodAccessor261.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-a02ff14f318a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrx_append\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lagy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'update_date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   1988\u001b[0m         \"\"\"\n\u001b[1;32m   1989\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1990\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1992\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"Expression 'update_date#2190' not supported within a window function.;;\\nProject [id_customer#2184, update_date#2190, action_flag#2204, filename#2194, load_date#2209, lagy#2269]\\n+- Project [id_customer#2184, update_date#2190, action_flag#2204, filename#2194, load_date#2209, _w0#2270, lag(_w0#2270, 1, null) AS lagy#2269]\\n   +- Project [id_customer#2184, update_date#2190, action_flag#2204, filename#2194, load_date#2209, _w0#2270]\\n      +- Project [id_customer#2184, update_date#2190, action_flag#2204, filename#2194, load_date#2209, _w0#2270, _w0#2270]\\n         +- Window [update_date#2190 windowspecdefinition(update_date#2190 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS _w0#2270], [update_date#2190 ASC NULLS FIRST]\\n            +- Project [id_customer#2184, update_date#2190, action_flag#2204, filename#2194, load_date#2209]\\n               +- Project [id_customer#2184, update_date#2190, action_flag#2204, filename#2194, cast(split(input_file_name(), /)[5] as date) AS load_date#2209]\\n                  +- Project [id_customer#2184, update_date#2190, CASE WHEN (action_flag#2199 = filling_in_preferences) THEN fill_in_preferences END AS action_flag#2204, filename#2194]\\n                     +- Project [id_customer#2184, update_date#2190, regexp_replace(lower(action_flag#2186),  , _) AS action_flag#2199, filename#2194]\\n                        +- Project [id_customer#2184, update_date#2190, action_flag#2186, input_file_name() AS filename#2194]\\n                           +- Project [id_customer#2184, cast(to_timestamp('update_date, None) as date) AS update_date#2190, action_flag#2186]\\n                              +- Relation[id_customer#2184,update_date#2185,action_flag#2186] csv\\n\""
     ]
    }
   ],
   "source": [
    "trx_append.withColumn('lagy', f.lag(f.col('update_date').over(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''CREATE EXTERNAL TABLE `digital_actions`(\n",
    "  id_customer string, \n",
    "  update_date date, \n",
    "  action_flag string)\n",
    "PARTITIONED BY ( \n",
    "  load_date date)\n",
    "  STORED AS PARQUET\n",
    " LOCATION 's3://formation-samba-us-west-2-seabright-samba-derived/clean/customer_profile' '''\n",
    "\n",
    "spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "dig_trx.write.format(\"parquet\")\\\n",
    ".mode('overwrite')\\\n",
    ".partitionBy('load_date')\\\n",
    ".option(\"path\", 's3://formation-samba-us-west-2-seabright-samba-derived/clean/digital_actions').saveAsTable(\"clean.digital_actions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s3://formation-samba-us-west-2-seabright-samba-sources/cross_sell/v1/2020-08-29/Base_Cross_Sell_New_Offers.csv', 's3://formation-samba-us-west-2-seabright-samba-sources/cross_sell/v1/2020-08-30/Base_Cross_Sell_New_Offers.csv', 's3://formation-samba-us-west-2-seabright-samba-sources/cross_sell/v1/2020-08-31/Base_Cross_Sell_New_Offers.csv', 's3://formation-samba-us-west-2-seabright-samba-sources/cross_sell/v1/2020-09-01/Base_Cross_Sell_New_Offers.csv', 's3://formation-samba-us-west-2-seabright-samba-sources/cross_sell/v1/2020-09-02/Base_Cross_Sell_New_Offers.csv', 's3://formation-samba-us-west-2-seabright-samba-sources/cross_sell/v1/2020-09-03/Base_Cross_Sell_New_Offers.csv', 's3://formation-samba-us-west-2-seabright-samba-sources/cross_sell/v1/2020-09-04/Base_Cross_Sell_New_Offers.csv', 's3://formation-samba-us-west-2-seabright-samba-sources/cross_sell/v1/2020-09-05/Base_Cross_Sell_New_Offers.csv', 's3://formation-samba-us-west-2-seabright-samba-sources/cross_sell/v1/2020-09-06/Base_Cross_Sell_New_Offers.csv', 's3://formation-samba-us-west-2-seabright-samba-sources/cross_sell/v1/2020-09-07/Base_Cross_Sell_New_Offers.csv', 's3://formation-samba-us-west-2-seabright-samba-sources/cross_sell/v1/2020-09-08/Base_Cross_Sell_New_Offers.csv', 's3://formation-samba-us-west-2-seabright-samba-sources/cross_sell/v1/2020-09-09/Base_Cross_Sell_New_Offers.csv', 's3://formation-samba-us-west-2-seabright-samba-sources/cross_sell/v1/2020-09-10/Base_Cross_Sell_New_Offers.csv', 's3://formation-samba-us-west-2-seabright-samba-sources/cross_sell/v1/2020-09-11/Base_Cross_Sell_New_Offers.csv', 's3://formation-samba-us-west-2-seabright-samba-sources/cross_sell/v1/2020-09-12/Base_Cross_Sell_New_Offers.csv']\n"
     ]
    }
   ],
   "source": [
    "daily_cross_path = \"s3://formation-samba-us-west-2-seabright-samba-sources/cross_sell/v1/*/*.csv\"\n",
    "daily_cross = spark.read.csv(daily_cross_path,header=True,sep=\",\")\n",
    "daily_cross = daily_cross.withColumn('load_date',f.split(f.input_file_name(),\"/\").getItem(5).cast('date'))\\\n",
    "            .withColumn('event_date',f.to_timestamp('event_date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_cross.write.format(\"parquet\")\\\n",
    ".partitionBy('load_date')\\\n",
    ".option(\"path\", 's3://formation-samba-us-west-2-seabright-samba-derived/clean/cross_sell').saveAsTable(\"clean.cross_sell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =\"s3://formation-samba-us-west-2-seabright-samba-sources/historical/2020-09-10/Base_Flight_Search_historical.csv\"\n",
    "hist_flight = spark.read.csv(path,header=True)\n",
    "hist_flight = hist_flight\\\n",
    ".withColumn('load_date',f.split(f.input_file_name(),\"/\").getItem(4).cast('date'))\\\n",
    " .withColumn('search_date',f.col('search_date').cast('date'))\\\n",
    "            .withColumn('departure_date',f.col('departure_date').cast('date'))\\\n",
    "            .withColumn('adults',f.col('adults').cast('int'))\\\n",
    "            .withColumn('children',f.col('children').cast('int'))\\\n",
    "                          .withColumn('infants',f.col('infants').cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_flight.select('search_date').distinct().sort(f.col('search_date').desc()).take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s3://formation-samba-us-west-2-seabright-samba-sources/flight_search/v1/2020-09-12/Base_Flight_Search.csv', 's3://formation-samba-us-west-2-seabright-samba-sources/flight_search/v1/2020-09-13/Base_Flight_Search.csv', 's3://formation-samba-us-west-2-seabright-samba-sources/flight_search/v1/2020-09-14/Base_Flight_Search.csv']\n"
     ]
    }
   ],
   "source": [
    "cmd = \"aws s3 ls --recursive  s3://formation-samba-us-west-2-seabright-samba-sources/flight_search/\"\n",
    "wow = subprocess.check_output(cmd.split())\n",
    "bsn = 's3://formation-samba-us-west-2-seabright-samba-sources/'\n",
    "daily_fl_path = [bsn+a.decode('utf-8') for a in wow.split() if a.decode('utf-8').endswith('csv')][4:]\n",
    "print(daily_fl_path[-3:])\n",
    "flight_search = spark.read.csv(daily_fl_path,header=True,sep=\",\")\n",
    "flight_search = flight_search.withColumn('load_date',f.split(f.input_file_name(),\"/\").getItem(5).cast('date'))\\\n",
    "            .withColumn('search_date',f.col('search_date').cast('date'))\\\n",
    "            .withColumn('departure_date',f.col('departure_date').cast('date'))\\\n",
    "            .withColumn('adults',f.col('adults').cast('int'))\\\n",
    ".withColumn('children',f.col('children').cast('int'))\\\n",
    ".withColumn('infants',f.col('infants').cast('int'))\\\n",
    ".filter(f.col('search_date')>f.lit('2020-09-08'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = flight_search.union(hist_flight).filter(f.col('id_customer')!='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_customer: string (nullable = true)\n",
      " |-- search_date: date (nullable = true)\n",
      " |-- channel: string (nullable = true)\n",
      " |-- adults: integer (nullable = true)\n",
      " |-- children: integer (nullable = true)\n",
      " |-- infants: integer (nullable = true)\n",
      " |-- departure_date: date (nullable = true)\n",
      " |-- origin_airport: string (nullable = true)\n",
      " |-- destination_airport: string (nullable = true)\n",
      " |-- origin_airport2: string (nullable = true)\n",
      " |-- destination_airport2: string (nullable = true)\n",
      " |-- trip_type: string (nullable = true)\n",
      " |-- load_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.write.format(\"parquet\")\\\n",
    ".partitionBy('load_date')\\\n",
    ".option(\"path\", 's3://formation-samba-us-west-2-seabright-samba-derived/clean/flight_search').saveAsTable(\"clean.flight_search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =\"s3://formation-samba-us-west-2-seabright-samba-sources/historical/Full_Table-2020-03-25/Base_Clube_Conta_Familia_Formation_FULL.CSV\"\n",
    "hist_club = spark.read.csv(path,header=True)\n",
    "hist_club = hist_club.withColumn('dt_solicitacao',f.to_timestamp('dt_solicitacao'))\\\n",
    "                        .withColumn('load_date',f.split(f.input_file_name(),\"/\").getItem(5).cast('date'))\n",
    "                       \n",
    "\n",
    "hist_club.select('dt_solicitacao').sort(f.col('dt_solicitacao').desc()).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s3://formation-samba-us-west-2-seabright-samba-sources/club-membership/v1/2020-09-14/Base_Clube_Conta_Familia_Formation.csv', 's3://formation-samba-us-west-2-seabright-samba-sources/club-membership/v1/2020-09-15/Base_Clube_Conta_Familia_Formation.csv', 's3://formation-samba-us-west-2-seabright-samba-sources/club-membership/v1/2020-09-16/Base_Clube_Conta_Familia_Formation.csv']\n"
     ]
    }
   ],
   "source": [
    "daily_club_path = \"s3://formation-samba-us-west-2-seabright-samba-sources/club-membership/v1/*/*.[csv|CSV]\"\n",
    "daily_club = spark.read.csv(daily_club_path,header=True,sep=\",\")\n",
    "daily_club = daily_club.withColumn('dt_solicitacao',f.to_timestamp('dt_solicitacao'))\\\n",
    "                        .withColumn('load_date',f.split(f.input_file_name(),\"/\").getItem(5).cast('date'))\\\n",
    "                        .filter(f.col('dt_solicitacao') > f.lit('2020-03-24'))\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "club = daily_club.union(hist_club).filter(f.col('id_customer')!='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "club.write.format(\"parquet\")\\\n",
    ".partitionBy('load_date')\\\n",
    ".option(\"path\", 's3://formation-samba-us-west-2-seabright-samba-derived/clean/club_membership')\\\n",
    ".saveAsTable(\"clean.club_membership\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
