{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/usr/lib/spark\")\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as func \n",
    "import pulp\n",
    "import __builtin__ as bt\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import DecimalType, StructType, StructField, IntegerType, StringType\n",
    "import decimal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import subprocess\n",
    "import uuid\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import DecimalType, StructType, StructField, IntegerType, StringType\n",
    "import decimal\n",
    "import re\n",
    "import __builtin__ as bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "latest_date ='20181112'\n",
    "\n",
    "offer_status = spark.read.csv(\"s3://stx-apollo-pr-datascience-internal/michael/test-cell/dash-offer-status-fix\", sep='|', header=True)\n",
    "\n",
    "\n",
    "measure1 = spark.read.csv(\"s3://stx-apollo-pr-datascience-internal/prod/optimization_v1.2/%s/measure/\" % latest_date, sep=\"|\") \\\n",
    "    .toDF( \\\n",
    "    \"sid\"\n",
    "    , \"cell_name\"\n",
    "    , \"revenue\"\n",
    "    , \"discount\"\n",
    "    , \"stars_awarded\"\n",
    "    , \"tier\"\n",
    "    , 'email_open_flag'\n",
    "    , 'customer_opted_in_flag'\n",
    "    , 'winner_flag'\n",
    "    , 'number_redemptions')\n",
    "\n",
    "\n",
    "\n",
    "measure = measure1.alias(\"a\").join(offer_status\n",
    "                               , [func.col(\"a.sid\") == offer_status.sid,\n",
    "                                  func.col(\"a.cell_name\") == offer_status.variant_name]\n",
    "                               , how='left') \\\n",
    "                               .select(func.col(\"a.*\"), 'offer_status') \\\n",
    "                               .where(\"offer_status is null or offer_status = 'offer'\")\\\n",
    "                               .drop('offer_status')\n",
    "\n",
    "measure = measure.withColumn(\"revenue\", measure[\"revenue\"].cast(DecimalType(8, 2))) \\\n",
    "    .withColumn(\"discount\", measure[\"discount\"].cast(DecimalType(8, 2))) \\\n",
    "    .withColumn(\"stars_awarded\", measure[\"stars_awarded\"].cast(DecimalType(8, 2)))\n",
    "\n",
    "measure.registerTempTable(\"measure\")\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Load segment_lookup\n",
    "\n",
    "segment_lookup = spark.read.csv(\"s3://stx-apollo-pr-datascience-internal/prod/optimization_v1.2/%s/segment_lookup/\" % latest_date, sep=\"|\") \\\n",
    "    .toDF(\n",
    "    \"segment_id\",\n",
    "    \"offer_type\",\n",
    "    \"segment\",\n",
    "    \"frequency_label\",\n",
    "    \"revenue_label\",\n",
    "    \"optin_label\",\n",
    "    \"highvalue_label\",\n",
    "    \"transaction_label\",\n",
    "    \"ampm_label\")\n",
    "\n",
    "segment_lookup_empty_str = segment_lookup.fillna(\"\")\n",
    "segment_lookup_empty_str.registerTempTable(\"segment_lookup_empty_str\")\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Load offer_lookup\n",
    "\n",
    "offer_lookup = spark.read.csv(\"s3://stx-apollo-pr-datascience-internal/prod/optimization_v1.2/%s/offer_lookup/\" % latest_date, sep=\"|\") \\\n",
    "    .toDF(\n",
    "    'offer_id'\n",
    "    , 'offer_type'\n",
    "    , 'product_variant'\n",
    "    , 'optin_required_flag'\n",
    "    , 'reward1_reward'\n",
    "    , 'reward2_reward'\n",
    "    , 'reward3_reward'\n",
    "    , 'hurdle1_int'\n",
    "    , 'hurdle2_int'\n",
    "    , 'hurdle3_int'\n",
    "    , 'hurdle1_up'\n",
    "    , 'hurdle2_up'\n",
    "    , 'hurdle3_up'\n",
    "    , 'duration'\n",
    "    , 'qualifying_daypart'\n",
    "    , 'product_category'\n",
    "    , 'reminder_bool')\n",
    "offer_lookup.registerTempTable(\"offer_lookup\")\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Load test variants\n",
    "\n",
    "variant_1temp = spark.read.csv(\"s3://stx-apollo-pr-datascience-internal/prod/optimization_v1.2/%s/variant/\" % latest_date, sep=\"|\") \\\n",
    "    .toDF(\n",
    "    'name'\n",
    "    , 'reward1_reward'\n",
    "    , 'reward2_reward'\n",
    "    , 'reward3_reward'\n",
    "    , 'hurdle1_int'\n",
    "    , 'hurdle2_int'\n",
    "    , 'hurdle3_int'\n",
    "    , 'hurdle1_up'\n",
    "    , 'hurdle2_up'\n",
    "    , 'product_variant'\n",
    "    , 'optin_required_flag'\n",
    "    , 'test_variant_flag'\n",
    "    , 'relevant_local_control_cell'\n",
    "    , 'start_date'\n",
    "    , 'offer_type'\n",
    "    , 'weekstartdate'\n",
    "    , 'hurdle3_up'\n",
    "    , 'duration'\n",
    "    , 'qualifying_daypart'\n",
    "    , 'product_category'\n",
    "    , 'reminder_bool')\n",
    "\n",
    "cols = variant_1temp.columns\n",
    "\n",
    "## Additional logic to replicate old dash 7-day, streak 7-day and Product Dash variants as\n",
    "## Dash 7-day 2-hurdle, Streak 7-day 3-hurdle, Product Dash 1-hurdle, respectively\n",
    "\n",
    "temp_pdash = variant_1temp.filter(variant_1temp.offer_type == 'Product Dash').withColumn(\n",
    "    \"offer_type\", func.lit(\"Product Dash 1-hurdle\")).select(*cols)\n",
    "temp_dash = variant_1temp.filter(variant_1temp.offer_type == 'Dash 7-day').withColumn(\"offer_type\",func.lit(\"Dash 7-day 2-hurdle\"))\\\n",
    "                .select(*cols)\n",
    "temp_streak = variant_1temp.filter(variant_1temp.offer_type == 'Streak 7-day').withColumn(\n",
    "    \"offer_type\", func.lit(\"Streak 7-day 3-hurdle\")).select(*cols)\n",
    "\n",
    "variant_temp_ = variant_1temp.unionAll(temp_dash).unionAll(temp_pdash).unionAll(temp_streak)\n",
    "\n",
    "#### NEW CODE BEGINS HERE ####\n",
    "# Get the distinct offer_types in the variants\n",
    "offer_types = [str(offer[0]) for offer in variant_temp_.select('offer_type').distinct().collect()]\n",
    "ctrl_frames = []\n",
    "\n",
    "def reLabelControlVariants(df, offer_type):\n",
    "    '''\n",
    "    Desc: This helper function selects all of the test variants in an offer type and\n",
    "          their relevant local control variants. The offer_type of the relevant local\n",
    "          control cell is then remapped to the offer_type in question.\n",
    "          This function will duplicate any existing control variants so a deduplication step is necessary.\n",
    "    Input: - The dataframe of the variants UNLOADED to \"s3://stx-apollo-pr-datascience-internal/prod/optimization_v1.2/{datetime}/variant\"\n",
    "           - offer_type as a String, e.g. \"Dash 7-day 2-hurdle\", \"Streak 7-day 3-hurdle\"\n",
    "    Output: A dataframe of the control variants needed for all of the test variants in the offer_type\n",
    "    '''\n",
    "    test_variants = df.where(\" offer_type = '{0}' and test_variant_flag = 1 \".format(offer_type) )\n",
    "    local_ctrl_names = [str(lc[0]) for lc in test_variants.select('relevant_local_control_cell').distinct().collect()]\n",
    "    ctrl_variants = df.filter( func.col('name').isin(local_ctrl_names) ) \\\n",
    "                                        .drop('offer_type') \\\n",
    "                                        .selectExpr(\"*\",\n",
    "                                                     \" '{0}' as offer_type\".format(offer_type)) \\\n",
    "                                        .select(*test_variants.columns)\n",
    "    return ctrl_variants\n",
    "\n",
    "#Apply the offer relabeling function to all of the control variants\n",
    "ctrl_frames = [reLabelControlVariants(variant_temp_, offer_type) for offer_type in offer_types]\n",
    "\n",
    "def unionMany(*dfs):\n",
    "    '''\n",
    "    Description: This helper function unions many dataframes together.\n",
    "    '''\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "# Unional all of the new control variants with the original variants\n",
    "variant_temp_reduced = unionMany(*ctrl_frames).unionAll(variant_temp_)\n",
    "\n",
    "\n",
    "# ctrl_frames =  reLabelControlVariants(variant_temp_, offer_types[0])\n",
    "#\n",
    "# for offer_type in offer_types[1:]:\n",
    "#     ctrl_frames = ctrl_frames.union(reLabelControlVariants(variant_temp_, offer_type))\n",
    "#\n",
    "# variant_temp_reduced = ctrl_frames.union(variant_temp_)\n",
    "\n",
    "# Remove any duplicated variants\n",
    "variant_temp = variant_temp_reduced.dropDuplicates()\n",
    "#### NEW CODE STOPS HERE ####\n",
    "\n",
    "variant_temp.registerTempTable(\"variant_temp\")\n",
    "\n",
    "variant = spark.sql('''\n",
    "\tselect\n",
    "\ta.*\n",
    "\t,b.offer_id\n",
    "\tfrom variant_temp a\n",
    "\tleft join offer_lookup b\n",
    "\ton a.offer_type <=> b.offer_type\n",
    "\t\tand a.product_variant <=> b.product_variant\n",
    "\t\tand a.optin_required_flag <=> b.optin_required_flag\n",
    "\t\tand a.reward1_reward <=> b.reward1_reward\n",
    "\t\tand a.reward2_reward <=> b.reward2_reward\n",
    "\t\tand a.reward3_reward <=> b.reward3_reward\n",
    "\t\tand a.hurdle1_int <=> b.hurdle1_int\n",
    "\t\tand a.hurdle2_int <=> b.hurdle2_int\n",
    "\t\tand a.hurdle3_int <=> b.hurdle3_int\n",
    "\t\tand a.hurdle1_up <=> b.hurdle1_up\n",
    "\t\tand a.hurdle2_up <=> b.hurdle2_up\n",
    "\t\tand a.hurdle3_up <=> b.hurdle3_up\n",
    "\t\tand a.duration <=> b.duration\n",
    "\t\tand a.qualifying_daypart <=> b.qualifying_daypart\n",
    "\t\tand trim(a.product_category) <=> trim(b.product_category)\n",
    "\t\tand a.reminder_bool <=> b.reminder_bool\n",
    "''')\n",
    "\n",
    "user_seg_new = spark.read.csv(\"s3://stx-apollo-pr-datascience-internal/prod/optimization_v1.2/%s/user_seg_new/\" % latest_date,header=True,sep=\"|\")\n",
    "model_user = user_seg_new.groupBy(\"segment_id\") \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed('count', 'user_count')\n",
    "\n",
    "user_seg = spark.read.csv(\"s3://stx-apollo-pr-datascience-internal/prod/optimization_v1.2/%s/user_seg/\" % latest_date,header=True,sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_seg.filter(func.col(\"segment_id\") == 1283).groupby(\"segment_id\",\"offer_id\").count().sort(\"offer_id\").show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_full_ = user_seg.join(variant, [user_seg.variant_name == variant.name])\\\n",
    "    .join(measure, [user_seg.sid == measure.sid, user_seg.variant_name == measure.cell_name])\\\n",
    "    .select(user_seg.sid,\n",
    "            user_seg.offer_id,\n",
    "            user_seg.segment_id,\n",
    "            variant.start_date,\n",
    "            variant.test_variant_flag,\n",
    "            measure.discount,\n",
    "            measure.revenue).distinct()\n",
    "user_revenue = user_full_.groupby(\"segment_id\",\"offer_id\",\"test_variant_flag\")\\\n",
    "            .agg(func.avg(\"revenue\").alias(\"avg_revenue\"),\n",
    "                 func.stddev_samp(\"revenue\").alias(\"std_revenue\"),\n",
    "                 func.count(\"revenue\").alias(\"cust_count\")).sort(\"offer_id\")\n",
    "user_revenue.filter(func.col(\"segment_id\") == 1283).show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_full_ = user_seg.join(variant, [user_seg.variant_name == variant.name])\\\n",
    "    .join(measure, [user_seg.sid == measure.sid, user_seg.variant_name == measure.cell_name])\\\n",
    "    .join(segment_lookup, [user_seg.segment_id == segment_lookup.segment_id])\\\n",
    "    .select(user_seg.sid,\n",
    "            user_seg.offer_id,\n",
    "            user_seg.segment_id,\n",
    "            variant.start_date,\n",
    "            variant.test_variant_flag,\n",
    "            segment_lookup.offer_type,\n",
    "            segment_lookup.frequency_label,\n",
    "            measure.discount,\n",
    "            measure.revenue).distinct()\n",
    "user_full_ = user_full_.fillna(\"na\",['frequency_label','offer_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =\"s3://stx-apollo-pr-datascience-internal/eosoba/optimization_v1.2/{0}/user_full\".format(latest_date)\n",
    "user_full_.write.parquet(path,mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =\"s3://stx-apollo-pr-datascience-internal/eosoba/optimization_v1.2/{0}/user_full\".format(latest_date)\n",
    "user_full_ = spark.read.parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_revenue = user_full_.groupby(\"segment_id\",\"offer_id\",\"test_variant_flag\")\\\n",
    "            .agg(func.avg(\"revenue\").alias(\"avg_revenue\"),\n",
    "                 func.stddev_samp(\"revenue\").alias(\"std_revenue\"),\n",
    "                 func.count(\"revenue\").alias(\"cust_count\")).sort(\"offer_id\")\n",
    "#user_revenue.filter(func.col(\"segment_id\") == 1283).show(100,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_revenue = user_full_.filter(func.col(\"revenue\")>0).select(\"offer_type\",\"frequency_label\",\"test_variant_flag\",\"revenue\")\\\n",
    "                                .groupby(\"offer_type\",\"frequency_label\",\"test_variant_flag\")\\\n",
    "                                 .agg(func.expr('percentile_approx(revenue,0.97)').alias('revenue_95_percentile'),\n",
    "                                     func.max('revenue').alias('max_revenue'))\n",
    "percc = spark.createDataFrame(percentile_revenue.rdd, percentile_revenue.schema)\n",
    "\n",
    "users_percentile = user_full_.alias(\"a\").join(percc.alias(\"b\"),\n",
    "                                              [func.col(\"a.frequency_label\") == func.col(\"b.frequency_label\"),\n",
    "                                               func.col(\"a.offer_type\") == func.col(\"b.offer_type\"),\n",
    "                                               func.col(\"a.test_variant_flag\") == func.col(\"b.test_variant_flag\")])\\\n",
    "                        .drop(func.col(\"b.frequency_label\")).drop(func.col(\"b.offer_type\")).drop(func.col(\"b.test_variant_flag\"))\\\n",
    "                        .withColumn(\"truncated_revenue\",func.when(func.col(\"revenue\") > func.col(\"revenue_95_percentile\"),\n",
    "                                            func.col(\"revenue_95_percentile\")).otherwise(func.col(\"revenue\")))\n",
    "\n",
    "user_revenue = users_percentile.groupby(\"segment_id\",\"offer_id\",\"test_variant_flag\")\\\n",
    "            .agg(func.avg(\"truncated_revenue\").alias(\"avg_revenue\"),\n",
    "                 func.max(\"max_revenue\").alias(\"max_revenue\"),\n",
    "                 func.max(\"revenue_95_percentile\").alias(\"revenue_95_percentile\"),\n",
    "                 func.stddev_samp(\"truncated_revenue\").alias(\"std_revenue\"),\n",
    "                 func.count(\"truncated_revenue\").alias(\"cust_count\"))\n",
    "\n",
    "\n",
    "uss = spark.createDataFrame(user_revenue.rdd, user_revenue.schema)\n",
    "uss.registerTempTable(\"uss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_users = user_full_.filter(func.col(\"test_variant_flag\") == 0)\n",
    "non_control_users = user_full_.filter(func.col(\"test_variant_flag\") == 1)\n",
    "\n",
    "offer_list = non_control_users.select(\"segment_id\",\"offer_id\",\"start_date\").distinct()\\\n",
    "                        .groupby(\"segment_id\",\"offer_id\")\\\n",
    "                        .agg(func.concat_ws(\",\",func.sort_array(func.collect_list(\"start_date\"))).alias(\"collected_start_date\"))\n",
    "\n",
    "user_full_non_control = non_control_users.alias(\"a\").join(offer_list.alias(\"b\"), [func.col(\"a.offer_id\") == func.col(\"b.offer_id\"),\n",
    "                                                                                  func.col(\"a.segment_id\") == func.col(\"b.segment_id\")])\\\n",
    "                                                .drop(func.col(\"b.offer_id\")).drop(func.col(\"b.segment_id\"))\n",
    "\n",
    "date_list = user_full_non_control.select(\"start_date\",\"segment_id\",\"collected_start_date\").distinct()\n",
    "\n",
    "user_full_control = control_users.alias(\"a\").join(date_list.alias(\"b\"), [func.col(\"a.start_date\") == func.col(\"b.start_date\"),\n",
    "                                                                        func.col(\"a.segment_id\") == func.col(\"b.segment_id\")])\\\n",
    "                                                .drop(func.col(\"b.start_date\")).drop(func.col(\"b.segment_id\"))\n",
    "user_full = user_full_non_control.union(user_full_control)\n",
    "\n",
    "percentile_revenue = user_full.filter(func.col(\"revenue\")>0).select(\"offer_type\",\"frequency_label\",\"test_variant_flag\",\"revenue\")\\\n",
    "                                .groupby(\"offer_type\",\"frequency_label\",\"test_variant_flag\")\\\n",
    "                                 .agg(func.expr('percentile_approx(revenue,0.97)').alias('revenue_95_percentile'),\n",
    "                                     func.max('revenue').alias('max_revenue'))\n",
    "percc = spark.createDataFrame(percentile_revenue.rdd, percentile_revenue.schema)\n",
    "\n",
    "users_percentile = user_full.alias(\"a\").join(percc.alias(\"b\"),\n",
    "                                              [func.col(\"a.frequency_label\") == func.col(\"b.frequency_label\"),\n",
    "                                               func.col(\"a.offer_type\") == func.col(\"b.offer_type\"),\n",
    "                                               func.col(\"a.test_variant_flag\") == func.col(\"b.test_variant_flag\")])\\\n",
    "                        .drop(func.col(\"b.frequency_label\")).drop(func.col(\"b.offer_type\")).drop(func.col(\"b.test_variant_flag\"))\\\n",
    "                        .withColumn(\"truncated_revenue\",func.when(func.col(\"revenue\") > func.col(\"revenue_95_percentile\"),\n",
    "                                            func.col(\"revenue_95_percentile\")).otherwise(func.col(\"revenue\")))\n",
    "\n",
    "user_revenue = users_percentile.groupby(\"collected_start_date\",\"segment_id\",\"offer_id\",\"test_variant_flag\")\\\n",
    "            .agg(func.avg(\"truncated_revenue\").alias(\"avg_revenue\"),\n",
    "                 func.stddev_samp(\"truncated_revenue\").alias(\"std_revenue\"),\n",
    "                 func.count(\"truncated_revenue\").alias(\"cust_count\"))\n",
    "\n",
    "uss = spark.createDataFrame(user_revenue.rdd, user_revenue.schema)\n",
    "uss.registerTempTable(\"uss\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_nir = spark.sql('''\n",
    "select\n",
    "\ta.segment_id\n",
    "\t,a.offer_id\n",
    "\t,a.avg_revenue as test_revenue\n",
    "\t,b.avg_revenue as control_revenue\n",
    "\t,a.std_revenue as test_std\n",
    "\t,b.std_revenue as control_std\n",
    "\t,a.cust_count as test_cust_count\n",
    "\t,b.cust_count as control_cust_count\n",
    "\t,(a.avg_revenue - b.avg_revenue) as nir\n",
    "\t,sqrt(pow(a.std_revenue,2)/a.cust_count + pow(b.std_revenue,2)/b.cust_count) as diff_std\n",
    "\t,(a.avg_revenue - b.avg_revenue) - 1.282 * sqrt(pow(a.std_revenue,2)/a.cust_count + pow(b.std_revenue,2)/b.cust_count) as lower_ci_limit\n",
    "\t,(a.avg_revenue - b.avg_revenue) + 1.282 * sqrt(pow(a.std_revenue,2)/a.cust_count + pow(b.std_revenue,2)/b.cust_count) as higher_ci_limit\n",
    "from uss a\n",
    "join uss b\n",
    "on a.segment_id = b.segment_id\n",
    "where a.test_variant_flag = 1\n",
    "\tand b.test_variant_flag = 0\n",
    "''')\n",
    "\n",
    "model_nir = user_nir.select(\"segment_id\"\n",
    "                            , \"offer_id\"\n",
    "                            , \"test_revenue\"\n",
    "                            , \"control_revenue\"\n",
    "                            , \"test_std\"\n",
    "                            , \"control_std\"\n",
    "                            , \"diff_std\"\n",
    "                            , \"test_cust_count\"\n",
    "                            , \"control_cust_count\"\n",
    "                            , \"nir\"\n",
    "                            , \"lower_ci_limit\"\n",
    "                            , \"higher_ci_limit\")\n",
    "\n",
    "user_full = user_seg.join(variant, [user_seg.variant_name == variant.name])\\\n",
    "    .join(measure, [user_seg.sid == measure.sid, user_seg.variant_name == measure.cell_name])\\\n",
    "    .select(user_seg.sid,\n",
    "            user_seg.offer_id,\n",
    "            user_seg.segment_id,\n",
    "            measure.tier,\n",
    "            func.when(measure.tier == 2, measure.stars_awarded).otherwise(func.lit(0)).alias(\"stars_awarded\")\n",
    "           )\n",
    "\n",
    "model_star =  user_full.groupby(\"offer_id\",\"segment_id\").agg(func.avg(\"stars_awarded\").alias(\"avg_stars_awarded\"))\n",
    "\n",
    "\n",
    "model_input = model_nir.join(model_star, [model_nir.segment_id == model_star.segment_id, model_nir.offer_id == model_star.offer_id])\\\n",
    "                        .join(model_user,[model_nir.segment_id == model_user.segment_id])\\\n",
    "                        .join(offer_lookup,[model_nir.offer_id == offer_lookup.offer_id])\\\n",
    "                        .select(model_nir.segment_id.alias(\"microsegment\")\n",
    "                                ,model_nir.offer_id.alias(\"offer\")\n",
    "                                ,model_nir.test_revenue\n",
    "                                ,model_nir.control_revenue\n",
    "                                ,model_nir.test_std\n",
    "                                ,model_nir.control_std\n",
    "                                ,model_nir.diff_std\n",
    "                                ,model_nir.test_cust_count.alias(\"previous_customers\")\n",
    "                                ,model_nir.control_cust_count.alias(\"local_control_customers\")\n",
    "                                ,model_nir.nir.alias(\"expected_nir\")\n",
    "                                ,model_nir.lower_ci_limit\n",
    "                                ,model_nir.higher_ci_limit\n",
    "                                ,model_star.avg_stars_awarded\n",
    "                                ,model_user.user_count.alias(\"customers\")\n",
    "                                ,offer_lookup.offer_type)\n",
    "\n",
    "df_input = model_input.toPandas()\n",
    "df_input.dtypes\n",
    "\n",
    "# pull in the global variables\n",
    "offer_type = df_input['offer_type'].unique().tolist()\n",
    "total_budget = range(2000000, 4000001, 500000)\n",
    "star_value = decimal.Decimal('0.037')\n",
    "\n",
    "# initialize a results data frame\n",
    "df_full_results = pd.DataFrame()\n",
    "\n",
    "# get current time and run id\n",
    "current = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "run_id = str(uuid.uuid4())\n",
    "\n",
    "# loop through the buckets and optimize\n",
    "# ------------------------------------------------------\n",
    "\n",
    "for i_offer_type in offer_type:\n",
    "    for i_total_budget in total_budget:\n",
    "        # filter out eligible offers\n",
    "        df_offer_full = df_input[(df_input['offer_type'] == i_offer_type)]\n",
    "\n",
    "        # and calculate the population-weighted cost, response & nir\n",
    "        df_offer_full['segment_cost'] = df_offer_full['customers'] * df_offer_full['avg_stars_awarded'] * star_value\n",
    "        df_offer_full['segment_nir'] = df_offer_full['customers'] * df_offer_full['expected_nir']\n",
    "\n",
    "        # add budget\n",
    "        df_offer_full['total_budget'] = i_total_budget\n",
    "        # add current time\n",
    "        df_offer_full['update_time'] = current\n",
    "        df_offer_full['run_id'] = run_id\n",
    "\n",
    "        # Apply business rules: filter out negative NIR and less than 200 previous customers\n",
    "        df_offer = df_offer_full[\n",
    "            (df_offer_full['expected_nir'] >= 0) & (df_offer_full['previous_customers'] >= 2000) & (\n",
    "            df_offer_full['local_control_customers'] >= 2000)]\n",
    "\n",
    "        # create list of all possible segment/offer combinations\n",
    "        combos = df_offer[['microsegment', 'offer']]\n",
    "        # combos.itertuples(index=False, name=None)\n",
    "        possible_offers = [tuple(x) for x in combos.itertuples(index=False, name=None)]\n",
    "\n",
    "        # dictionary of expected offer segment nir\n",
    "        combos = df_offer[['microsegment', 'offer', 'segment_nir']]\n",
    "        # combos.itertuples(index=False, name=None)\n",
    "        offer_nir_values = {x[0:2]: x[2] for x in combos.itertuples(index=False, name=None)}\n",
    "\n",
    "        # dictionary of expected offer costs\n",
    "        combos = df_offer[['microsegment', 'offer', 'segment_cost']]\n",
    "        # combos.itertuples(index=False, name=None)\n",
    "        offer_costs = {x[0:2]: x[2] for x in combos.itertuples(index=False, name=None)}\n",
    "\n",
    "        o = pulp.LpVariable.dicts('', possible_offers,\n",
    "                                  lowBound=0,\n",
    "                                  upBound=1,\n",
    "                                  cat=pulp.LpInteger)\n",
    "\n",
    "        # formulate and solve the optimization problem via pulp\n",
    "        # ------------------------------------------------------\n",
    "\n",
    "        # create the problem\n",
    "        prob = pulp.LpProblem('maximize nir', pulp.LpMaximize)\n",
    "\n",
    "        # add the objective function first\n",
    "        prob += bt.sum([float(offer_nir_values[offer]) * o[offer] for offer in\n",
    "                        possible_offers])  # <--- eval statement switches values used in objective function\n",
    "\n",
    "        # add in the total budget constraint\n",
    "        prob += bt.sum([float(offer_costs[offer]) * o[offer] for offer in possible_offers]) <= i_total_budget, ''\n",
    "\n",
    "        # each segment gets exactly one offer\n",
    "        segments = df_offer['microsegment'].tolist()\n",
    "        for segment in segments:\n",
    "            offers = df_offer[(df_offer['microsegment'] == segment)]['offer'].tolist()\n",
    "            prob += pulp.lpSum([o[(segment, offer)] for offer in offers]) == 1, ''\n",
    "\n",
    "        # write the problem to an lp file\n",
    "        # prob.writeLP(path+'offer_optimization_nir('+str(i_offer_type)+'_'+str(i_total_budget)+').lp')\n",
    "\n",
    "        # solve the lp\n",
    "        prob.solve()\n",
    "\n",
    "        # print the results status\n",
    "        print 'status nir optimization (' + str(i_offer_type) + ', ' + str(i_total_budget) + '): ' + pulp.LpStatus[\n",
    "            prob.status].lower()\n",
    "\n",
    "        # save the decision variables with their optimum value\n",
    "        results = {}\n",
    "        for v in prob.variables():\n",
    "            if str(v.name) != \"__dummy\":\n",
    "                results[eval(str(v.name).replace(\"_\", \"\"))] = v.varValue\n",
    "\n",
    "        # convert decision variable dictionary to a dataframe\n",
    "        df_opt_results = pd.DataFrame.from_dict(results, orient='index')\n",
    "    \n",
    "        df_opt_results.columns = ['chosen_nir']\n",
    "\n",
    "        # process the output\n",
    "        # ------------------------------------------------------\n",
    "\n",
    "        # join the results to the original full data frame\n",
    "        df_opt_results.reset_index(drop=False, inplace=True)\n",
    "        df_opt_results.columns = ['index_full', 'chosen']\n",
    "        df_opt_results[['microsegment', 'offer']] = df_opt_results.index_full.apply(pd.Series)\n",
    "        df_opt_results.drop('index_full', axis=1, inplace=True)\n",
    "\n",
    "        df_full_results_interim = pd.merge(df_offer_full, df_opt_results, how='left', on=['microsegment', 'offer'])\n",
    "        df_full_results_interim['optimization_status'] = pulp.LpStatus[prob.status].lower()\n",
    "\n",
    "        # and combine with the other optimization runs\n",
    "        df_full_results = pd.concat([df_full_results, df_full_results_interim])\n",
    "\n",
    "        df_full_results = df_full_results[[\n",
    "                    'microsegment'\n",
    "                    , 'offer'\n",
    "                    , 'expected_nir'\n",
    "                    , 'avg_stars_awarded'\n",
    "                    , 'customers'\n",
    "                    , 'previous_customers'\n",
    "                    , 'local_control_customers'\n",
    "                    , 'offer_type'\n",
    "                    , 'chosen'\n",
    "                    , 'segment_cost'\n",
    "                    , 'segment_nir'\n",
    "                    , 'total_budget'\n",
    "                    , 'optimization_status'\n",
    "                    , 'update_time'\n",
    "                    , 'run_id'\n",
    "                    , 'test_revenue'\n",
    "                    , 'control_revenue'\n",
    "                    , 'test_std'\n",
    "                    , 'control_std'\n",
    "                    , 'lower_ci_limit'\n",
    "                    , 'higher_ci_limit'\n",
    "                    , 'diff_std']]\n",
    "\n",
    "# save results\n",
    "# ------------------------------------------------------\n",
    "\n",
    "df_full_results_to_save_temp = spark.createDataFrame(df_full_results)\n",
    "df_full_results_to_save = df_full_results_to_save_temp.select(\n",
    "\t\t\"microsegment\"\n",
    "\t\t,\"offer\"\n",
    "\t\t,\"expected_nir\"\n",
    "\t\t,\"avg_stars_awarded\"\n",
    "\t\t,\"customers\"\n",
    "\t\t,\"previous_customers\"\n",
    "\t\t,\"local_control_customers\"\n",
    "\t\t,\"offer_type\"\n",
    "\t\t,func.regexp_replace(\"chosen\", 'NaN', '').alias(\"chosen\")\n",
    "\t\t,\"segment_cost\"\n",
    "\t\t,\"segment_nir\"\n",
    "\t\t,\"total_budget\"\n",
    "\t\t,\"optimization_status\"\n",
    "\t\t,\"update_time\"\n",
    "\t\t,\"run_id\"\n",
    "\t\t,\"test_revenue\"\n",
    "\t\t,\"control_revenue\"\n",
    "\t\t,func.regexp_replace(\"test_std\", 'NaN', '').alias(\"test_std\")\n",
    "\t\t,func.regexp_replace(\"control_std\", 'NaN', '').alias(\"control_std\")\n",
    "\t\t,func.regexp_replace(\"lower_ci_limit\", 'NaN', '').alias(\"lower_ci_limit\")\n",
    "\t\t,func.regexp_replace(\"higher_ci_limit\", 'NaN', '').alias(\"higher_ci_limit\")\n",
    "\t\t,func.regexp_replace(\"diff_std\", 'NaN', '').alias(\"diff_std\"))\n",
    "\n",
    "outfile_df_full_results_to_save = \"s3://stx-apollo-pr-datascience-internal/eosoba/optimization_v1.2/%s/final_result_outlier_97/\" % latest_date\n",
    "subprocess.call([\"aws\", \"s3\", \"rm\", outfile_df_full_results_to_save, \"--recursive\"])\n",
    "df_full_results_to_save.repartition(1).write.csv(path=outfile_df_full_results_to_save, sep='|', header='true',\n",
    "                                                 nullValue='', quote=None,mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = spark.read.csv(\"s3://stx-apollo-pr-datascience-internal/prod/optimization_v1.2/%s/final_result\" % latest_date,header=True,sep=\"|\")\n",
    "path = \"s3://stx-apollo-pr-datascience-internal/eosoba/optimization_v1.2/%s/final_result/\" % latest_date\n",
    "fn.repartition(1).write.csv(path,header=True,sep=\"|\",nullValue='', quote=None,mode=\"overwrite\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:92: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:97: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status nir optimization (Dash 7-day, 2000000): optimal\n",
      "status nir optimization (Dash 7-day, 2500000): optimal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:178: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status nir optimization (Dash 7-day, 3000000): optimal\n",
      "status nir optimization (Dash 7-day, 3500000): optimal\n",
      "status nir optimization (Dash 7-day, 4000000): optimal\n",
      "status nir optimization (Streak 7-day 1-hurdle, 2000000): optimal\n",
      "status nir optimization (Streak 7-day 1-hurdle, 2500000): optimal\n",
      "status nir optimization (Streak 7-day 1-hurdle, 3000000): optimal\n",
      "status nir optimization (Streak 7-day 1-hurdle, 3500000): optimal\n",
      "status nir optimization (Streak 7-day 1-hurdle, 4000000): optimal\n",
      "status nir optimization (Streak 7-day 3-hurdle, 2000000): optimal\n",
      "status nir optimization (Streak 7-day 3-hurdle, 2500000): optimal\n",
      "status nir optimization (Streak 7-day 3-hurdle, 3000000): optimal\n",
      "status nir optimization (Streak 7-day 3-hurdle, 3500000): optimal\n",
      "status nir optimization (Streak 7-day 3-hurdle, 4000000): optimal\n",
      "status nir optimization (Dash 7-day 2-hurdle, 2000000): optimal\n",
      "status nir optimization (Dash 7-day 2-hurdle, 2500000): optimal\n",
      "status nir optimization (Dash 7-day 2-hurdle, 3000000): optimal\n",
      "status nir optimization (Dash 7-day 2-hurdle, 3500000): optimal\n",
      "status nir optimization (Dash 7-day 2-hurdle, 4000000): optimal\n",
      "status nir optimization (Product Dash 1-hurdle, 2000000): optimal\n",
      "status nir optimization (Product Dash 1-hurdle, 2500000): optimal\n",
      "status nir optimization (Product Dash 1-hurdle, 3000000): optimal\n",
      "status nir optimization (Product Dash 1-hurdle, 3500000): optimal\n",
      "status nir optimization (Product Dash 1-hurdle, 4000000): optimal\n",
      "status nir optimization (Dash 7-day 1-hurdle, 2000000): optimal\n",
      "status nir optimization (Dash 7-day 1-hurdle, 2500000): optimal\n",
      "status nir optimization (Dash 7-day 1-hurdle, 3000000): optimal\n",
      "status nir optimization (Dash 7-day 1-hurdle, 3500000): optimal\n",
      "status nir optimization (Dash 7-day 1-hurdle, 4000000): optimal\n",
      "status nir optimization (Menu Quest 2-Item, 2000000): optimal\n",
      "status nir optimization (Menu Quest 2-Item, 2500000): optimal\n",
      "status nir optimization (Menu Quest 2-Item, 3000000): optimal\n",
      "status nir optimization (Menu Quest 2-Item, 3500000): optimal\n",
      "status nir optimization (Menu Quest 2-Item, 4000000): optimal\n",
      "status nir optimization (Dash 7-day 3-hurdle, 2000000): optimal\n",
      "status nir optimization (Dash 7-day 3-hurdle, 2500000): optimal\n",
      "status nir optimization (Dash 7-day 3-hurdle, 3000000): optimal\n",
      "status nir optimization (Dash 7-day 3-hurdle, 3500000): optimal\n",
      "status nir optimization (Dash 7-day 3-hurdle, 4000000): optimal\n",
      "status nir optimization (Product Dash, 2000000): optimal\n",
      "status nir optimization (Product Dash, 2500000): optimal\n",
      "status nir optimization (Product Dash, 3000000): optimal\n",
      "status nir optimization (Product Dash, 3500000): optimal\n",
      "status nir optimization (Product Dash, 4000000): optimal\n",
      "status nir optimization (Streak 7-day 2-hurdle, 2000000): optimal\n",
      "status nir optimization (Streak 7-day 2-hurdle, 2500000): optimal\n",
      "status nir optimization (Streak 7-day 2-hurdle, 3000000): optimal\n",
      "status nir optimization (Streak 7-day 2-hurdle, 3500000): optimal\n",
      "status nir optimization (Streak 7-day 2-hurdle, 4000000): optimal\n",
      "status nir optimization (Menu Quest, 2000000): optimal\n",
      "status nir optimization (Menu Quest, 2500000): optimal\n",
      "status nir optimization (Menu Quest, 3000000): optimal\n",
      "status nir optimization (Menu Quest, 3500000): optimal\n",
      "status nir optimization (Menu Quest, 4000000): optimal\n",
      "status nir optimization (Product Dash 2-hurdle, 2000000): optimal\n",
      "status nir optimization (Product Dash 2-hurdle, 2500000): optimal\n",
      "status nir optimization (Product Dash 2-hurdle, 3000000): optimal\n",
      "status nir optimization (Product Dash 2-hurdle, 3500000): optimal\n",
      "status nir optimization (Product Dash 2-hurdle, 4000000): optimal\n",
      "status nir optimization (Streak 7-day, 2000000): optimal\n",
      "status nir optimization (Streak 7-day, 2500000): optimal\n",
      "status nir optimization (Streak 7-day, 3000000): optimal\n",
      "status nir optimization (Streak 7-day, 3500000): optimal\n",
      "status nir optimization (Streak 7-day, 4000000): optimal\n",
      "status nir optimization (Menu Quest 5-Item, 2000000): optimal\n",
      "status nir optimization (Menu Quest 5-Item, 2500000): optimal\n",
      "status nir optimization (Menu Quest 5-Item, 3000000): optimal\n",
      "status nir optimization (Menu Quest 5-Item, 3500000): optimal\n",
      "status nir optimization (Menu Quest 5-Item, 4000000): optimal\n",
      "status nir optimization (Product Dash 3-hurdle, 2000000): optimal\n",
      "status nir optimization (Product Dash 3-hurdle, 2500000): optimal\n",
      "status nir optimization (Product Dash 3-hurdle, 3000000): optimal\n",
      "status nir optimization (Product Dash 3-hurdle, 3500000): optimal\n",
      "status nir optimization (Product Dash 3-hurdle, 4000000): optimal\n",
      "status nir optimization (Menu Quest 4-Item, 2000000): optimal\n",
      "status nir optimization (Menu Quest 4-Item, 2500000): optimal\n",
      "status nir optimization (Menu Quest 4-Item, 3000000): optimal\n",
      "status nir optimization (Menu Quest 4-Item, 3500000): optimal\n",
      "status nir optimization (Menu Quest 4-Item, 4000000): optimal\n",
      "status nir optimization (Menu Quest 3-Item, 2000000): optimal\n",
      "status nir optimization (Menu Quest 3-Item, 2500000): optimal\n",
      "status nir optimization (Menu Quest 3-Item, 3000000): optimal\n",
      "status nir optimization (Menu Quest 3-Item, 3500000): optimal\n",
      "status nir optimization (Menu Quest 3-Item, 4000000): optimal\n"
     ]
    }
   ],
   "source": [
    "user_nir = spark.sql('''\n",
    "select\n",
    "\ta.segment_id\n",
    "\t,a.collected_start_date\n",
    "\t,a.offer_id\n",
    "\t,a.avg_revenue as test_revenue\n",
    "\t,b.avg_revenue as control_revenue\n",
    "\t,a.std_revenue as test_std\n",
    "\t,b.std_revenue as control_std\n",
    "\t,a.cust_count as test_cust_count\n",
    "\t,b.cust_count as control_cust_count\n",
    "\t,(a.avg_revenue - b.avg_revenue) as nir\n",
    "\t,sqrt(pow(a.std_revenue,2)/a.cust_count + pow(b.std_revenue,2)/b.cust_count) as diff_std\n",
    "\t,(a.avg_revenue - b.avg_revenue) - 1.282 * sqrt(pow(a.std_revenue,2)/a.cust_count + pow(b.std_revenue,2)/b.cust_count) as lower_ci_limit\n",
    "\t,(a.avg_revenue - b.avg_revenue) + 1.282 * sqrt(pow(a.std_revenue,2)/a.cust_count + pow(b.std_revenue,2)/b.cust_count) as higher_ci_limit\n",
    "from uss a\n",
    "join uss b\n",
    "on a.segment_id = b.segment_id and  a.collected_start_date = b.collected_start_date\n",
    "where a.test_variant_flag = 1 and b.test_variant_flag = 0\n",
    "''')\n",
    "\n",
    "\n",
    "user_full = user_seg.join(variant, [user_seg.variant_name == variant.name])\\\n",
    "    .join(measure, [user_seg.sid == measure.sid, user_seg.variant_name == measure.cell_name])\\\n",
    "    .select(user_seg.sid,\n",
    "            user_seg.offer_id,\n",
    "            user_seg.segment_id,\n",
    "            measure.tier,\n",
    "            func.when(measure.tier == 2, measure.stars_awarded).otherwise(func.lit(0)).alias(\"stars_awarded\")\n",
    "           )\n",
    "\n",
    "model_star =  user_full.groupby(\"offer_id\",\"segment_id\").agg(func.avg(\"stars_awarded\").alias(\"avg_stars_awarded\"))\n",
    "\n",
    "\n",
    "model_nir = user_nir.select(\"segment_id\"\n",
    "                            ,\"collected_start_date\"\n",
    "                            , \"offer_id\"\n",
    "                            , \"test_revenue\"\n",
    "                            , \"control_revenue\"\n",
    "                            , \"test_std\"\n",
    "                            , \"control_std\"\n",
    "                            , \"diff_std\"\n",
    "                            , \"test_cust_count\"\n",
    "                            , \"control_cust_count\"\n",
    "                            , \"nir\"\n",
    "                            , \"lower_ci_limit\"\n",
    "                            , \"higher_ci_limit\")\n",
    "\n",
    "\n",
    "model_input = model_nir.join(model_star, [model_nir.segment_id == model_star.segment_id, model_nir.offer_id == model_star.offer_id])\\\n",
    "                        .join(model_user,[model_nir.segment_id == model_user.segment_id])\\\n",
    "                        .join(offer_lookup,[model_nir.offer_id == offer_lookup.offer_id])\\\n",
    "                        .select(model_nir.segment_id.alias(\"microsegment\")\n",
    "                                ,model_nir.offer_id.alias(\"offer\")\n",
    "                                ,model_nir.collected_start_date\n",
    "                                ,model_nir.test_revenue\n",
    "                                ,model_nir.control_revenue\n",
    "                                ,model_nir.test_std\n",
    "                                ,model_nir.control_std\n",
    "                                ,model_nir.diff_std\n",
    "                                ,model_nir.test_cust_count.alias(\"previous_customers\")\n",
    "                                ,model_nir.control_cust_count.alias(\"local_control_customers\")\n",
    "                                ,model_nir.nir.alias(\"expected_nir\")\n",
    "                                ,model_nir.lower_ci_limit\n",
    "                                ,model_nir.higher_ci_limit\n",
    "                                ,model_star.avg_stars_awarded\n",
    "                                ,model_user.user_count.alias(\"customers\")\n",
    "                                ,offer_lookup.offer_type)\n",
    "\n",
    "\n",
    "\n",
    "df_input = model_input.toPandas()\n",
    "df_input.dtypes\n",
    "\n",
    "# pull in the global variables\n",
    "offer_type = df_input['offer_type'].unique().tolist()\n",
    "total_budget = range(2000000, 4000001, 500000)\n",
    "star_value = decimal.Decimal('0.037')\n",
    "\n",
    "# initialize a results data frame\n",
    "df_full_results = pd.DataFrame()\n",
    "\n",
    "# get current time and run id\n",
    "current = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "run_id = str(uuid.uuid4())\n",
    "\n",
    "# loop through the buckets and optimize\n",
    "# ------------------------------------------------------\n",
    "\n",
    "for i_offer_type in offer_type:\n",
    "    for i_total_budget in total_budget:\n",
    "        # filter out eligible offers\n",
    "        df_offer_full = df_input[(df_input['offer_type'] == i_offer_type)]\n",
    "\n",
    "        # and calculate the population-weighted cost, response & nir\n",
    "        df_offer_full['segment_cost'] = df_offer_full['customers'] * df_offer_full['avg_stars_awarded'] * star_value\n",
    "        df_offer_full['segment_nir'] = df_offer_full['customers'] * df_offer_full['expected_nir']\n",
    "\n",
    "        # add budget\n",
    "        df_offer_full['total_budget'] = i_total_budget\n",
    "        # add current time\n",
    "        df_offer_full['update_time'] = current\n",
    "        df_offer_full['run_id'] = run_id\n",
    "\n",
    "        # Apply business rules: filter out negative NIR and less than 200 previous customers\n",
    "        df_offer = df_offer_full[\n",
    "            (df_offer_full['expected_nir'] >= 0) & (df_offer_full['previous_customers'] >= 2000) & (\n",
    "            df_offer_full['local_control_customers'] >= 2000)]\n",
    "\n",
    "        # create list of all possible segment/offer combinations\n",
    "        combos = df_offer[['microsegment', 'offer']]\n",
    "        # combos.itertuples(index=False, name=None)\n",
    "        possible_offers = [tuple(x) for x in combos.itertuples(index=False, name=None)]\n",
    "\n",
    "        # dictionary of expected offer segment nir\n",
    "        combos = df_offer[['microsegment', 'offer', 'segment_nir']]\n",
    "        # combos.itertuples(index=False, name=None)\n",
    "        offer_nir_values = {x[0:2]: x[2] for x in combos.itertuples(index=False, name=None)}\n",
    "\n",
    "        # dictionary of expected offer costs\n",
    "        combos = df_offer[['microsegment', 'offer', 'segment_cost']]\n",
    "        # combos.itertuples(index=False, name=None)\n",
    "        offer_costs = {x[0:2]: x[2] for x in combos.itertuples(index=False, name=None)}\n",
    "\n",
    "        o = pulp.LpVariable.dicts('', possible_offers,\n",
    "                                  lowBound=0,\n",
    "                                  upBound=1,\n",
    "                                  cat=pulp.LpInteger)\n",
    "\n",
    "        # formulate and solve the optimization problem via pulp\n",
    "        # ------------------------------------------------------\n",
    "\n",
    "        # create the problem\n",
    "        prob = pulp.LpProblem('maximize nir', pulp.LpMaximize)\n",
    "\n",
    "        # add the objective function first\n",
    "        prob += bt.sum([float(offer_nir_values[offer]) * o[offer] for offer in\n",
    "                        possible_offers])  # <--- eval statement switches values used in objective function\n",
    "\n",
    "        # add in the total budget constraint\n",
    "        prob += bt.sum([float(offer_costs[offer]) * o[offer] for offer in possible_offers]) <= i_total_budget, ''\n",
    "\n",
    "        # each segment gets exactly one offer\n",
    "        segments = df_offer['microsegment'].tolist()\n",
    "        for segment in segments:\n",
    "            offers = df_offer[(df_offer['microsegment'] == segment)]['offer'].tolist()\n",
    "            prob += pulp.lpSum([o[(segment, offer)] for offer in offers]) == 1, ''\n",
    "\n",
    "        # write the problem to an lp file\n",
    "        # prob.writeLP(path+'offer_optimization_nir('+str(i_offer_type)+'_'+str(i_total_budget)+').lp')\n",
    "\n",
    "        # solve the lp\n",
    "        prob.solve()\n",
    "\n",
    "        # print the results status\n",
    "        print 'status nir optimization (' + str(i_offer_type) + ', ' + str(i_total_budget) + '): ' + pulp.LpStatus[\n",
    "            prob.status].lower()\n",
    "\n",
    "        # save the decision variables with their optimum value\n",
    "        results = {}\n",
    "        for v in prob.variables():\n",
    "            if str(v.name) != \"__dummy\":\n",
    "                results[eval(str(v.name).replace(\"_\", \"\"))] = v.varValue\n",
    "\n",
    "        # convert decision variable dictionary to a dataframe\n",
    "        df_opt_results = pd.DataFrame.from_dict(results, orient='index')\n",
    "    \n",
    "        df_opt_results.columns = ['chosen_nir']\n",
    "\n",
    "        # process the output\n",
    "        # ------------------------------------------------------\n",
    "\n",
    "        # join the results to the original full data frame\n",
    "        df_opt_results.reset_index(drop=False, inplace=True)\n",
    "        df_opt_results.columns = ['index_full', 'chosen']\n",
    "        df_opt_results[['microsegment', 'offer']] = df_opt_results.index_full.apply(pd.Series)\n",
    "        df_opt_results.drop('index_full', axis=1, inplace=True)\n",
    "\n",
    "        df_full_results_interim = pd.merge(df_offer_full, df_opt_results, how='left', on=['microsegment', 'offer'])\n",
    "        df_full_results_interim['optimization_status'] = pulp.LpStatus[prob.status].lower()\n",
    "\n",
    "        # and combine with the other optimization runs\n",
    "        df_full_results = pd.concat([df_full_results, df_full_results_interim])\n",
    "\n",
    "        df_full_results = df_full_results[[\n",
    "                    'microsegment'\n",
    "                    , 'offer'\n",
    "                    , 'expected_nir'\n",
    "                    , 'avg_stars_awarded'\n",
    "                    , 'customers'\n",
    "                    , 'previous_customers'\n",
    "                    , 'local_control_customers'\n",
    "                    , 'offer_type'\n",
    "                    , 'chosen'\n",
    "                    , 'collected_start_date'\n",
    "                    , 'segment_cost'\n",
    "                    , 'segment_nir'\n",
    "                    , 'total_budget'\n",
    "                    , 'optimization_status'\n",
    "                    , 'update_time'\n",
    "                    , 'run_id'\n",
    "                    , 'test_revenue'\n",
    "                    , 'control_revenue'\n",
    "                    , 'test_std'\n",
    "                    , 'control_std'\n",
    "                    , 'lower_ci_limit'\n",
    "                    , 'higher_ci_limit'\n",
    "                    , 'diff_std']]\n",
    "\n",
    "# save results\n",
    "# ------------------------------------------------------\n",
    "\n",
    "df_full_results_to_save_temp = spark.createDataFrame(df_full_results)\n",
    "df_full_results_to_save = df_full_results_to_save_temp.select(\n",
    "\t\t\"microsegment\"\n",
    "\t\t,\"offer\"\n",
    "\t\t,\"expected_nir\"\n",
    "\t\t,\"avg_stars_awarded\"\n",
    "\t\t,\"customers\"\n",
    "\t\t,\"previous_customers\"\n",
    "\t\t,\"local_control_customers\"\n",
    "\t\t,\"offer_type\"\n",
    "\t\t,func.regexp_replace(\"chosen\", 'NaN', '').alias(\"chosen\")\n",
    "        ,\"collected_start_date\"\n",
    "\t\t,\"segment_cost\"\n",
    "\t\t,\"segment_nir\"\n",
    "\t\t,\"total_budget\"\n",
    "\t\t,\"optimization_status\"\n",
    "\t\t,\"update_time\"\n",
    "\t\t,\"run_id\"\n",
    "\t\t,\"test_revenue\"\n",
    "\t\t,\"control_revenue\"\n",
    "\t\t,func.regexp_replace(\"test_std\", 'NaN', '').alias(\"test_std\")\n",
    "\t\t,func.regexp_replace(\"control_std\", 'NaN', '').alias(\"control_std\")\n",
    "\t\t,func.regexp_replace(\"lower_ci_limit\", 'NaN', '').alias(\"lower_ci_limit\")\n",
    "\t\t,func.regexp_replace(\"higher_ci_limit\", 'NaN', '').alias(\"higher_ci_limit\")\n",
    "\t\t,func.regexp_replace(\"diff_std\", 'NaN', '').alias(\"diff_std\"))\n",
    "\n",
    "outfile_df_full_results_to_save = \"s3://stx-apollo-pr-datascience-internal/eosoba/optimization_v1.2/%s/final_result_control_logic_outlier_97/\" % latest_date\n",
    "subprocess.call([\"aws\", \"s3\", \"rm\", outfile_df_full_results_to_save, \"--recursive\"])\n",
    "df_full_results_to_save.repartition(1).write.csv(path=outfile_df_full_results_to_save, sep='|', header='true',\n",
    "                                                 nullValue='', quote=None,mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =\"s3://stx-apollo-pr-datascience-internal/eosoba/optimization_v1.2/{0}/final_result_outlier_positive\".format(latest_date)\n",
    "ff = spark.read.csv(path,header=True,sep=\"|\")\n",
    "pp = path+\"_1\"\n",
    "ff.repartition(1).write.csv(pp,header=True,sep=\"|\",mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3://stx-apollo-pr-datascience-internal/eosoba/optimization_v1.2/20181105/final_result_control_logic_outlier/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
